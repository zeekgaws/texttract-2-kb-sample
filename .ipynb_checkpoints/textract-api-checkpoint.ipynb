{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377cae8e-8e75-49e8-932a-e7109e15e41d",
   "metadata": {},
   "source": [
    "# Document Layout Aware Processing and Retrieval Augmented Generation.\n",
    "\n",
    "This notebook was tested on a SageMaker Studio Notebook `Data Science 3.0` kernel and  `ml.t3.xlarge` instance.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Background](#Background-(Problem-Description-and-Approach))\n",
    "1. [Document Extraction](#Document-Extraction)\n",
    "1. [Document Processing](#Document-Processing)\n",
    "1. [Document Chunking](#Document-Chunking)\n",
    "1. [Indexing](#Indexing)\n",
    "1. [RAG](#RAG)\n",
    "1. [CleanUp](#CleanUp)\n",
    "1. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead3d274-9977-44be-91e2-bf3b4bbbb745",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This example notebook guides you through the process of utilizing Amazon Textract's layout feature. This feature allows you to extract content from your document while maintaining its layout and reading format. Amazon Textract Layout feature is able to detect the following sections:\n",
    "- Titles\n",
    "- Headers\n",
    "- Sub-headers\n",
    "- Text\n",
    "- Tables\n",
    "- Figures\n",
    "- List \n",
    "- Footers\n",
    "- Page Numbers\n",
    "- Key-Value pairs\n",
    "\n",
    "Here is a snippet of Textract Layout feature on a page of Amazon Sustainability report using the Textract Console UI:\n",
    "<img src=\"images/amazonsus2022.jpg\" width=\"1000\"/>\n",
    "\n",
    "The [Amazon Textract Textractor Library](https://aws-samples.github.io/amazon-textract-textractor/index.html) is a library that seamlessly works with Textract features to aid in document processing. You can start by checking out the [examples in the documentation.](https://aws-samples.github.io/amazon-textract-textractor/notebooks/layout_analysis_for_text_linearization.html)\n",
    "This notebook utilizes the Textractor library to interact with Amazon Textract and interpret its response. It enriches the extracted document text with XML tags to delineate sections, facilitating layout-aware chunking and document indexing into a Vector Database (DB). This process aims to enhance Retrieval Augmented Generation (RAG) performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Background (Problem Description and Approach)\n",
    "\n",
    "- **Problem statement**: \n",
    "RAG serves as a technique aimed at enhancing the effectiveness of Large Language Models (LLMs) on lengthy textual content. While widely adopted, implementing RAG necessitates initial processing to extract and segment text into meaningful chunks, especially challenging for intricate assets like PDFs. Many document parsing approaches overlook layout semantics or use simplistic methods like fixed window carving, lacking awareness of document structure or elements. This can disrupt contextual continuity and diminish the performance of RAG systems. An optimal RAG input pipeline would intelligently divide PDF texts into vectorized segments aligned with layout and content semantics, preserving informational integrity for the LLM. In essence, a context-aware parsing phase is pivotal for enabling RAG techniques to realize their full potential, particularly when handling extensive or intricate documents.\n",
    "\n",
    "- **Our approach**: \n",
    "\n",
    "<img src=\"images/txt layout-Page-2.jpg\" width=\"800\"/>\n",
    "\n",
    "1. Upload multi-page document to Amazon S3.\n",
    "2. Call Amazon Textract Start Document Analysis api call to extract Document Text including Layout and Tables. The response provides structured text aligned with the original document formatting and the pandas tables of each table detected in the document.\n",
    "3. Enrich this extracted text further with XML tags indicating semantic sections, adding contextual metadata through the Textractor library.\n",
    "4. The textrcat library extracts tables in plain text, maintaining their original layout. However, for improved processing and manipulation, it's advisable to convert them to CSV format. This method replaces the plain text tables with their CSV counterparts obtained from Textract's table feature.\n",
    "5.  In this approach, the extracted text is segmented based on document title sections, the highest hierarchy level in a document. Each subsection within the title section is then chunked according to a maximum word threshold. Below outlines our approach to handling the chunking of subsection elements.:\n",
    "\n",
    "    - **Tables:** Tables are chunked row by row until the maximum number of alphanumeric words is reached. For each table chunk, the column headers are added to the table along with the table header, typically the sentence or paragraph preceding the table in the document. This ensures that the information of the table is retained in each chunk.\n",
    "    \n",
    "    <img src=\"images/table chunkers.png\" width=\"800\" height=700/>\n",
    "    \n",
    "        To handle tables with merged cells, this solution first unmerges any merged cell ranges, then duplicates the original merged cell value into each of the corresponding individual cells after unmerging.\n",
    "    \n",
    "    <img src=\"images/complex-tables.png\" width=\"800\" height=700/>\n",
    "    \n",
    "    - **List:** Chunking lists found in documents can be challenging. Naive chunking methods often split list items by sentence or newline characters. However, this approach presents issues as only the first list chunk typically contains the list title, which provides essential information about the list items. Consequently, subsequent list chunks become obsolete. In this notebook, lists are chunked based on their individual list items. Additionally, the header of the list is appended to each list chunk to ensure that the information of the list is preserved in each chunk.\n",
    "    <img src=\"images/list chunker.png\" width=\"800\" height=700/>\n",
    "    \n",
    "    - **Section and subsection:** The structure of a document can generally be categorized into titles, sections, and paragraphs. A paragraph is typically the smallest unit of a document that conveys information independently, particularly within the context of a section or subsection header. In this method, text sections are chunked based on paragraphs, and the section header is added to each paragraph chunk (as well as tables and lists) within that section of the document.\n",
    "    <img src=\"images/text chunks.png\" width=\"800\" height=700/>\n",
    "    \n",
    "6. Metadata is appended to each respective chunk during indexing, encompassing:\n",
    "    - The entire CSV tables detected within the chunk.\n",
    "    - The section header ID associated with the chunk.\n",
    "    - The section title ID linked to the chunk.\n",
    "    \n",
    "    When retrieving a passage based on hybrid search (combining semantic and text matching), there's flexibility in the amount of content forwarded to the LLM. Some queries may necessitate additional information, allowing users to choose whether to send the corresponding chunk subsection or title section based on the specific use case.\n",
    "\n",
    " *Some chunk may exceed the fixed word count threshold due to preserving paragraphs and dealing with complex tables. \n",
    "\n",
    "**Prerequisite:**\n",
    "- [Amazon Bedrock model access](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html)\n",
    "- [Deploy Embedding and Text Generation Large Language Models with SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-use.html)\n",
    "- Amazon OpenSearch Cluster (Provisioned Cluster or Serverless):\n",
    "    - [Create OpenSearch Service Domain](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/createupdatedomains.html). This solution uses **IAM** as master user for fine-grained access control. **NOTE:** This solution only works with Amazon Opensearch Service version 2.11 and higher.\n",
    "    OR\n",
    "    - [Create OpenSearch Serverless Collection](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-getting-started.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba85156-5ae0-4e53-8aaf-0ec4ef1d7e2b",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e75351-bc80-4416-a8d2-e2be0cadb07b",
   "metadata": {},
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa108161-6352-4d2d-b2c9-cad24879b579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall amazon-textract-textractor==1.7.11\n",
    "!pip install inflect\n",
    "!pip install requests-aws4auth\n",
    "!pip install opensearch-py\n",
    "!pip install anthropic\n",
    "%pip install -U opensearch-py==2.3.1\n",
    "%pip install -U boto3==1.33.2\n",
    "%pip install -U retrying==1.3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63129e-f3c9-41ea-bf48-3d2608a2531a",
   "metadata": {},
   "source": [
    "Restart the Kernel \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc711dd9-37b2-4db6-9187-9d095cb511f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d14636c-4f02-4b2d-874a-cc44de0c85c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from textractor import Textractor\n",
    "from textractor.visualizers.entitylist import EntityList\n",
    "from textractor.data.constants import TextractFeatures\n",
    "import io\n",
    "import inflect\n",
    "import pprint\n",
    "from utility import create_bedrock_execution_role, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss\n",
    "import random\n",
    "from retrying import retry\n",
    "suffix = random.randrange(200, 900)\n",
    "from collections import OrderedDict\n",
    "import boto3\n",
    "import time\n",
    "import openpyxl\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from openpyxl.cell import Cell\n",
    "from openpyxl.worksheet.cell_range import CellRange\n",
    "s3=boto3.client(\"s3\")\n",
    "from botocore.config import Config\n",
    "config = Config(\n",
    "    read_timeout=600, \n",
    "    retries = dict(\n",
    "        max_attempts = 5 \n",
    "    )\n",
    ")\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime',region_name='us-west-2',config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f454779c-f2e2-4cdf-9bbe-b0eeb1152877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from utility import create_bedrock_execution_role, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss\n",
    "import random\n",
    "from retrying import retry\n",
    "suffix = random.randrange(200, 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20575c4d-15d6-41af-86ef-a4c6cb2c9c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region_name)\n",
    "service = 'aoss'\n",
    "s3_client = boto3.client('s3')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "s3_suffix = f\"{region_name}-{account_id}\"\n",
    "bucket_name = f'textract-2-kb-ws-{s3_suffix}' # replace it with your bucket name.\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecdb5683-e711-4078-8fe2-fc72dfb6a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3bucket = s3_client.create_bucket(\n",
    "    Bucket=bucket_name,\n",
    "    CreateBucketConfiguration={ 'LocationConstraint': region_name }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2fefa-df80-48ec-8cfa-17c1099039f1",
   "metadata": {},
   "source": [
    "Utility functions are provided for embedding generation using select models from Amazon SageMaker Jumpstart and Amazon Bedrock. These models were chosen arbitrarily, but you have the flexibility to customize them by using models of your preference available on Bedrock, SageMaker JumpStart, or HuggingFace.\\\n",
    "**Change the placeholders for sagemaker endpoint names for the respective models below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8144b0ae-4209-4a0d-aa71-6fa8097d1653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This dictionary `model_dimension_mapping` maps different model names to their respective embedding dimensions.\n",
    "\"\"\"\n",
    "model_dimension_mapping={\"titanv2\":1024,\"titanv1\":1536,\"bge\":1024,\"all-mini-lm\":384,\"e5\":1024}\n",
    "\n",
    "def _get_emb_(passage, model):\n",
    "    \"\"\"\n",
    "    This function takes a passage of text and a model name as input, and returns the corresponding text embedding.\n",
    "    The function first checks the provided model name and then invokes the appropriate model or API to generate the text embedding.  \n",
    "    After invoking the appropriate model or API, the function extracts the text embedding from the response and returns it.\n",
    "    \"\"\"\n",
    "\n",
    "    if \"titanv1\" in model:\n",
    "        response = bedrock_runtime.invoke_model(body=json.dumps({\"inputText\":passage}),\n",
    "                                    modelId=\"amazon.titan-embed-text-v1\", \n",
    "                                    accept=\"application/json\", \n",
    "                                    contentType=\"application/json\")\n",
    "\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        embedding=response_body['embedding']\n",
    "    elif \"titanv2\" in model:\n",
    "        response = bedrock_runtime.invoke_model(body=json.dumps({\"inputText\":passage,\"dimensions\":1024,\"normalize\":False}),\n",
    "                                    modelId=\"amazon.titan-embed-text-v2:0\", \n",
    "                                    accept=\"application/json\", \n",
    "                                    contentType=\"application/json\")\n",
    "\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        embedding=response_body['embedding']\n",
    "    elif \"all-mini-lm\" in model:\n",
    "        payload = {'text_inputs': [passage]}\n",
    "        payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "        response = SAGEMAKER.invoke_endpoint(EndpointName=\"SAGEMAKER JUMPSTART ALL MINI LM V6 ENDPOINT\", \n",
    "                                                    ContentType='application/json',  \n",
    "                                                    Body=payload)\n",
    "\n",
    "        model_predictions = json.loads(response['Body'].read())\n",
    "        embedding = model_predictions['embedding'][0]\n",
    "    elif \"e5\" in model:\n",
    "        payload = {\"text_inputs\":[passage],\"mode\":\"embedding\"} #{'text_inputs': [passage]}\n",
    "        payload = json.dumps(payload).encode('utf-8')\n",
    "        response = SAGEMAKER.invoke_endpoint(EndpointName=\"SAGEMAKER JUMPSTART E5 ENDPOINT\", \n",
    "                                                    ContentType='application/json',  \n",
    "                                                    Body=payload)\n",
    "\n",
    "        model_predictions = json.loads(response['Body'].read())\n",
    "        embedding = model_predictions['embedding'][0]\n",
    "    elif \"bge\" in model:\n",
    "        payload = {\"text_inputs\":[passage],\"mode\":\"embedding\"} #{'text_inputs': [passage]}\n",
    "        payload = json.dumps(payload).encode('utf-8')\n",
    "        response = SAGEMAKER.invoke_endpoint(EndpointName=\"SAGEMAKER JUMPSTART BGE ENDPOINT\", \n",
    "                                                    ContentType='application/json',  \n",
    "                                                    Body=payload)\n",
    "\n",
    "        model_predictions = json.loads(response['Body'].read())\n",
    "        embedding = model_predictions['embedding'][0]\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f9a42",
   "metadata": {},
   "source": [
    "Utility function to inference Anthropic Claude models on Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5351c441-f635-412d-9318-0a3b5b55cbe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bedrock_streemer(response):\n",
    "    stream = response.get('body')\n",
    "    answer = \"\"\n",
    "    i = 1\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')\n",
    "            if  chunk:\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                if \"delta\" in chunk_obj:                    \n",
    "                    delta = chunk_obj['delta']\n",
    "                    if \"text\" in delta:\n",
    "                        text=delta['text'] \n",
    "                        print(text, end=\"\")\n",
    "                        answer+=str(text)       \n",
    "                        i+=1\n",
    "                if \"amazon-bedrock-invocationMetrics\" in chunk_obj:\n",
    "                    input_tokens= chunk_obj['amazon-bedrock-invocationMetrics']['inputTokenCount']\n",
    "                    output_tokens=chunk_obj['amazon-bedrock-invocationMetrics']['outputTokenCount']\n",
    "                    print(f\"\\nInput Tokens: {input_tokens}\\nOutput Tokens: {output_tokens}\")\n",
    "    return answer,input_tokens, output_tokens\n",
    "\n",
    "def bedrock_claude_(chat_history,system_message, prompt,model_id,image_path=None):\n",
    "    content=[]\n",
    "    if image_path:       \n",
    "        if not isinstance(image_path, list):\n",
    "            image_path=[image_path]      \n",
    "        for img in image_path:\n",
    "            s3 = boto3.client('s3')\n",
    "            match = re.match(\"s3://(.+?)/(.+)\", img)\n",
    "            image_name=os.path.basename(img)\n",
    "            _,ext=os.path.splitext(image_name)\n",
    "            if \"jpg\" in ext: ext=\".jpeg\"                        \n",
    "            if match:\n",
    "                bucket_name = match.group(1)\n",
    "                key = match.group(2)    \n",
    "                obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                base_64_encoded_data = base64.b64encode(obj['Body'].read())\n",
    "                base64_string = base_64_encoded_data.decode('utf-8')\n",
    "            content.extend([{\"type\":\"text\",\"text\":image_name},{\n",
    "              \"type\": \"image\",\n",
    "              \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": f\"image/{ext.lower().replace('.','')}\",\n",
    "                \"data\": base64_string\n",
    "              }\n",
    "            }])\n",
    "    \n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt\n",
    "            })\n",
    "    chat_history.append({\"role\": \"user\",\n",
    "            \"content\": content})\n",
    "    prompt = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1500,\n",
    "        \"temperature\": 0.1,\n",
    "        \"system\":system_message,\n",
    "        \"messages\": chat_history\n",
    "    }\n",
    "    answer = \"\"\n",
    "    prompt = json.dumps(prompt)\n",
    "    response = bedrock_runtime.invoke_model_with_response_stream(body=prompt, modelId=model_id, accept=\"application/json\", contentType=\"application/json\")\n",
    "    answer,input_tokens,output_tokens=bedrock_streemer(response) \n",
    "    return answer, input_tokens, output_tokens\n",
    "\n",
    "def _invoke_bedrock_with_retries(current_chat, chat_template, question, model_id, image_path):\n",
    "    max_retries = 5\n",
    "    backoff_base = 2\n",
    "    max_backoff = 3  # Maximum backoff time in seconds\n",
    "    retries = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response,input_tokens,output_tokens = bedrock_claude_(current_chat, chat_template, question, model_id, image_path)\n",
    "            return response,input_tokens,output_tokens\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'ThrottlingException':\n",
    "                if retries < max_retries:\n",
    "                    # Throttling, exponential backoff\n",
    "                    sleep_time = min(max_backoff, backoff_base ** retries + random.uniform(0, 1))\n",
    "                    time.sleep(sleep_time)\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    raise e\n",
    "            elif e.response['Error']['Code'] == 'ModelStreamErrorException':\n",
    "                if retries < max_retries:\n",
    "                    # Throttling, exponential backoff\n",
    "                    sleep_time = min(max_backoff, backoff_base ** retries + random.uniform(0, 1))\n",
    "                    time.sleep(sleep_time)\n",
    "                    retries += 1\n",
    "                else:\n",
    "                    raise e\n",
    "            else:\n",
    "                # Some other API error, rethrow\n",
    "                raise\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7d155-4e4f-4fca-860f-bce713ceee42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Document Extraction\n",
    "We employ the Amazon 2024 10K report as an example document. Using the textractor library, we trigger the Amazon Textract `start document analysis` API to initiate an asynchronous process for extracting document text and identifying additional elements like document layout and tables.\\\n",
    "Change **BUCKET** placeholder to your bucket name in Amazon S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c1d6899-82bd-49aa-8d64-ff4ebfdb235b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET=bucket_name\n",
    "extractor = Textractor(region_name=\"us-west-2\")\n",
    "file=\"underwriting_guide.pdf\" #Change to file path either in S3 or Local\n",
    "doc_id= os.path.basename(file)\n",
    "file_name, ext = os.path.splitext(file)\n",
    "if file.startswith(\"s3://\"):\n",
    "    document = extractor.start_document_analysis(\n",
    "        file_source=file,\n",
    "        features=[TextractFeatures.LAYOUT,TextractFeatures.TABLES],\n",
    "        # client_request_token=doc_id,\n",
    "        save_image=False,\n",
    "        s3_output_path=f\"s3://{BUCKET}/textract-output/{file_name}/\"\n",
    "  \n",
    "    )\n",
    "else:\n",
    "    document = extractor.start_document_analysis(\n",
    "        file_source=file,\n",
    "        features=[TextractFeatures.LAYOUT,TextractFeatures.TABLES],\n",
    "        # client_request_token=doc_id,\n",
    "        save_image=False,\n",
    "        s3_upload_path=f\"s3://{BUCKET}\",\n",
    "        s3_output_path=f\"s3://{BUCKET}/textract-output/{file_name}/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efebf919-47a1-462d-8f21-e7ffe23cf6c3",
   "metadata": {},
   "source": [
    "By leveraging the Textractor linearization function, we enhance the extracted content with XML tags while concealing certain page sections such as headers, footers, and non-essential images.\n",
    "\n",
    "We opt to tag tables, lists, title sections, and sub-sections to facilitate the efficient identification and chunking of these document elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f086761-25bd-4535-b1b1-10724abceae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health\n",
      "\n",
      "Underwriting\n",
      " Guide\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "<titles><<title>><title>Health Insurance Build Charts </title><</title>>\n",
      "\n",
      "<<list>><list>1. If there has been weight loss of more than 20 pounds within one year, divide the loss in half and add it to current weight before entering into the table. \n",
      "2. A reduction in rating due to build will be considered once an insured loses enough to qualify for the lower rating and maintains the reduced weight for at least 6-12 months. \n",
      "3. Underweight can be more serious than overweight. Keep in mind that in certain people, because of small physical stature, an underweight condition is normal and perfectly healthy. \n",
      "4. Sudden weight loss without voluntary dieting is an ominous sign. \n",
      "5. Certain conditions require an additional rating because of the enhanced morbidity risk, e.g., hypertension and overweight build. \n",
      "6. The weight is in pounds. </list><</list>>\n",
      "\n",
      "\n",
      "\n",
      "<tables><table>Height\t\tHeight MALE\t\t\t\t\t\tHeight MALE Height\t\tHeight MALE Height FEMALE\t\t\t\t\t\n",
      "Height MALE Height FEMALE F E E T\tHeight MALE Height FEMALE F I E N E C T H\tHeight MALE Height FEMALE F I 20% for E N Weights E C less T H than\tHeight MALE Height FEMALE F I 20% for E N Weights E C less Avg. T H than Weight\tHeight MALE Height FEMALE F I 20% for Percentage Increase in Premium E N Weights E C less Avg. T H than Weight\t\t\t\tHeight MALE Height FEMALE F I 20% for Percentage Increase in Premium F E N Weights E E C less Avg. E T H than Weight T\tHeight MALE Height FEMALE F I 20% for Percentage Increase in Premium F I E N Weights E N E C less Avg. E C T H than Weight T H\tHeight MALE Height FEMALE F I 20% for Percentage Increase in Premium F I 20% for E N Weights E N Weights E C less Avg. E C less T H than Weight T H than\tHeight MALE Height FEMALE F I 20% for Percentage Increase in Premium F I 20% for E N Weights E N Weights E C less Avg. E C less Avg. T H than Weight T H than Weight\tHeight MALE Height FEMALE F I 20% for Percentage Increase in Premium F I 20% for Percentage Increase in Premium E N Weights E N Weights E C less Avg. E C less Avg. T H than Weight T H than Weight\t\t\t\n",
      "\t\t\t\t20%\t40%\t80% +ER\tDecline\t\t\t\t\t20%\t40%\t80% +ER\tDecline\n",
      "5\t0\t90\t129\t167-183\t184-195\t196-208\t209+\t4\t8\t75\t107\t148-163\t164-173\t174-184\t185+\n",
      "5\t1\t93\t133\t173-189\t190-201\t202-214\t215+\t4\t9\t77\t110\t153-168\t169-178\t179-189\t190+\n",
      "5\t2\t97\t138\t180-196\t197-209\t210-223\t224+\t4\t10\t79\t113\t157-172\t173-182\t183-194\t195+\n",
      "5\t3\t100\t143\t186-203\t204-216\t217-231\t232+\t4\t11\t81\t115\t160-175\t176-185\t186-198\t199+\n",
      "5\t4\t103\t147\t192-209\t210-222\t223-237\t238+\t5\t0\t83\t118\t164-180\t181-191\t192-203\t204+\n",
      "5\t5\t106\t151\t197-215\t216-229\t230-244\t245+\t5\t1\t85\t121\t169-185\t186-196\t197-208\t209+\n",
      "5\t6\t109\t156\t204-222\t223-236\t237-252\t253+\t5\t2\t87\t124\t173-189\t190-201\t202-214\t215+\n",
      "5\t7\t112\t160\t210-228\t229-242\t243-258\t259+\t5\t3\t90\t128\t179-196\t197-207\t208-220\t221+\n",
      "5\t8\t116\t165\t216-235\t236-250\t251-266\t267+\t5\t4\t92\t131\t183-200\t201-212\t213-226\t227+\n",
      "5\t9\t119\t170\t223-243\t244-258\t259-274\t275+\t5\t5\t94\t134\t188-205\t206-217\t218-231\t232+\n",
      "5\t10\t122\t174\t229-248\t249-264\t265-281\t282+\t5\t6\t96\t137\t192-210\t211-222\t223-236\t237+\n",
      "5\t11\t125\t179\t235-256\t257-272\t273-289\t290+\t5\t7\t99\t141\t198-216\t217-229\t230-243\t244+\n",
      "6\t0\t128\t184\t242-263\t264-279\t280-297\t298+\t5\t8\t102\t145\t204-222\t223-235\t236-250\t251+\n",
      "6\t1\t131\t190\t250-272\t273-289\t290-307\t308+\t5\t9\t105\t150\t211-230\t231-244\t245-259\t260+\n",
      "6\t2\t134\t195\t257-279\t280-296\t297-315\t316+\t5\t10\t107\t153\t215-235\t236-248\t249-264\t265+\n",
      "6\t3\t138\t201\t265-287\t288-306\t307-325\t326+\t5\t11\t111\t159\t224-244\t245-258\t259-274\t275+\n",
      "6\t4\t142\t206\t272-295\t296-313\t314-333\t334+\t6\t0\t115\t164\t231-252\t253-267\t268-283\t284+\n",
      "6\t5\t145\t211\t279-302\t303-321\t322-341\t342+\t6\t1\t118\t168\t237-258\t259-273\t274-290\t291+\n",
      "6\t6\t150\t217\t287-311\t312-330\t331-351\t352+\t6\t2\t120\t172\t243-264\t265-280\t281-297\t298+\n",
      "6\t7\t152\t223\t295-319\t320-339\t340-360\t361+\t6\t3\t123\t176\t249-271\t272-286\t287-303\t304+\n",
      "6\t8\t158\t228\t302-327\t328-347\t348-368\t369+\t6\t4\t127\t181\t256-278\t279-295\t296-312\t313+\n",
      "</table>\n",
      "\n",
      "\n",
      "\n",
      "The Health Insurance Build Chart is a guide to the rating action World Insurance Company will take regarding weight The percentage increases assume that there are no other impairments present. If other impairments are found, the judgment of the underwriter will determine what action will be taken. Weights greater than those in the chart will render an applicant uninsurable for health coverage. \n",
      "\n",
      "The Health Insurance Build Chart is for use only with insureds and dependents age 15 or over. Cases involving overweight dependents under age 15 will be considered indi- vidually by the underwriter in consultation with the Medical Director. \n",
      "\n",
      "Premium increases because of weight will be reconsidered in accordance with the following guidelines: \n",
      "\n",
      "<<list>><list>1. If the increase is 20% or less, the certificate/policy must be in force at least six months. \n",
      "2. If the increase is more than 20%, the certificate/policy must be in force at least one year. \n",
      "3. In either case the weight loss must be maintained for more than six months. World must be furnished, at the expense of the insured, a statement from a physician or testing facility showing the insured's current height and weight. </list><</list>>\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textractor.data.text_linearization_config import TextLinearizationConfig\n",
    "\n",
    "config = TextLinearizationConfig(\n",
    "    hide_figure_layout=False,\n",
    "    title_prefix=\"<titles><<title>><title>\",\n",
    "    title_suffix=\"</title><</title>>\",\n",
    "    hide_header_layout=True,\n",
    "    section_header_prefix=\"<headers><<header>><header>\",\n",
    "    section_header_suffix=\"</header><</header>>\",\n",
    "    table_prefix=\"<tables><table>\",\n",
    "    table_suffix=\"</table>\",\n",
    "    list_layout_prefix=\"<<list>><list>\",\n",
    "    list_layout_suffix=\"</list><</list>>\",\n",
    "    hide_footer_layout=True,\n",
    "    hide_page_num_layout=True,\n",
    ")\n",
    "\n",
    "print(document.pages[3].get_text(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a89081-022e-4451-b3be-38fd4f697e0d",
   "metadata": {},
   "source": [
    "## Document Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05fa55-fe4e-462e-9908-b8cae7d731d2",
   "metadata": {},
   "source": [
    "This code snippet comprises a Python function `split_list_items_` and a script segment that processes a document containing tables and text, converting tables into CSV format and maintaining the document structure with text and tables.\n",
    "\n",
    "The function `split_list_items_` takes a string as input, likely representing a document with nested lists marked by specific XML tags. It parses this string, extracting items and handling nested lists appropriately. The function then returns a list containing the extracted items.\n",
    "\n",
    "The script segment following the function processes each page of the document. It identifies tables, converts them to CSV format, and wraps them with XML tags for identification. If lists are present in the document, the script utilizes the `split_list_items_` function to handle them. The processed content is stored in dictionaries for further use.\n",
    "\n",
    "The `layout_table_to_excel` loads a pandas dataframe in excel format to handle spanned columns/rows in complex tables. It duplicates the spanned row/columns value across corresponding spanned cells to help keep the intergrity of complex tables.\n",
    "\n",
    "This script segment efficiently manages document content, ensuring tables are properly formatted while preserving the document's structure with text and lists. It serves to handle data extraction and processing tasks involving documents with mixed content types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7211dead-9643-4afb-b9d0-cc59f905ce1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def strip_newline(cell):\n",
    "    \"\"\"\n",
    "    A utility function to strip newline characters from a cell.\n",
    "    Parameters:\n",
    "    cell (str): The cell value.\n",
    "    Returns:\n",
    "    str: The cell value with newline characters removed.\n",
    "    \"\"\"\n",
    "    return str(cell).strip()\n",
    "\n",
    "def layout_table_to_excel(document, ids,csv_seperator):    \n",
    "    \"\"\"\n",
    "    Converts an Excel table from a document to a Pandas DataFrame, \n",
    "    handling duplicated values across merged cells.\n",
    "\n",
    "    Args:\n",
    "        document: Document containing Excel table \n",
    "        ids: ID of the Excel table in the document\n",
    "        csv_seperator: Separator for CSV string conversion\n",
    "\n",
    "    Returns: \n",
    "        Pandas DataFrame representation of the Excel table\n",
    "    \"\"\"\n",
    "    # save the table in excel format to preserve the structure of any merged cells\n",
    "    buffer = io.BytesIO()    \n",
    "    document.tables[ids].to_excel(buffer)\n",
    "    buffer.seek(0)\n",
    "    # Load workbook, get active worksheet\n",
    "    wb = openpyxl.load_workbook(buffer)\n",
    "    worksheet = wb.active\n",
    "    # Unmerge cells, duplicate merged values to individual cells\n",
    "    all_merged_cell_ranges: list[CellRange] = list(\n",
    "            worksheet.merged_cells.ranges\n",
    "        )\n",
    "    for merged_cell_range in all_merged_cell_ranges:\n",
    "        merged_cell: Cell = merged_cell_range.start_cell\n",
    "        worksheet.unmerge_cells(range_string=merged_cell_range.coord)\n",
    "        for row_index, col_index in merged_cell_range.cells:\n",
    "            cell: Cell = worksheet.cell(row=row_index, column=col_index)\n",
    "            cell.value = merged_cell.value\n",
    "    # determine table header index\n",
    "    df = pd.DataFrame(worksheet.values)\n",
    "    df=df.map(strip_newline)\n",
    "    df0=df.to_csv(sep=csv_seperator,index=False, header=None)\n",
    "    row_count=len([x for x in df0.split(\"\\n\") if x])\n",
    "    if row_count>1:\n",
    "        if not all(value.strip() == '' for value in df0.split(\"\\n\")[0].split(csv_seperator)): \n",
    "            row_count=1\n",
    "    # attach table column names\n",
    "    column_row=0 if row_count==1 else 1\n",
    "    df.columns = df.iloc[column_row] \n",
    "    df = df[column_row+1:]\n",
    "    return df\n",
    "\n",
    "def split_list_items_(items):\n",
    "    \"\"\"\n",
    "    Splits the given string into a list of items, handling nested lists.\n",
    "\n",
    "    Parameters:\n",
    "    items (str): The input string containing items and possibly nested lists.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the items extracted from the input string.\n",
    "    \"\"\"\n",
    "    parts = re.split(\"(<<list>><list>|</list><</list>>)\", items)  \n",
    "    output = []\n",
    "\n",
    "    inside_list = False\n",
    "    list_item = \"\"\n",
    "\n",
    "    for p in parts:\n",
    "        if p == \"<<list>><list>\":\n",
    "            inside_list = True    \n",
    "            list_item=p\n",
    "        elif p == \"</list><</list>>\":\n",
    "            inside_list = False\n",
    "            list_item += p\n",
    "            output.append(list_item)\n",
    "            list_item = \"\" \n",
    "        elif inside_list:\n",
    "            list_item += p.strip()\n",
    "        else:\n",
    "            output.extend(p.split('\\n'))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a011ade1-bd57-49e3-a531-297e3c34d4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\"\"\"\n",
    "This script processes a document containing tables and text. It converts the tables into CSV format \n",
    "and wraps them with XML tags for easy identification. The document structure with text and tables is maintained.\n",
    "\"\"\"\n",
    "csv_seperator=\"|\" #\"\\t\"\n",
    "document_holder={}\n",
    "table_page={}\n",
    "count=0\n",
    "# Whether to handle merged cells by duplicating merged value across corresponding individual cells\n",
    "unmerge_span_cells=True \n",
    "# Loop through each page in the document\n",
    "for ids,page in enumerate(document.pages):\n",
    "    table_count=len([word for word in page.get_text(config=config).split() if \"<tables><table>\" in word]) # get the number of table in the extracted document page by header we set earlier\n",
    "    assert table_count==len(page.tables) # check that number of tables per page is same as *tables extracted by textract TABLE feature\n",
    "    content=page.get_text(config=config).split(\"<tables>\")\n",
    "    document_holder[ids]=[]    \n",
    "    for idx,item in enumerate(content):\n",
    "        if \"<table>\" in item:           \n",
    "            if unmerge_span_cells:\n",
    "                df=layout_table_to_excel(document, count,csv_seperator)\n",
    "            else:\n",
    "                df0=  document.tables[count].to_pandas(use_columns=False).to_csv(header=False, index=None,sep=csv_seperator)\n",
    "                row_count=len([x for x in df0.split(\"\\n\") if x]) #Check the number of rows in the parsed table to determine how to read the table headers. if table row count is 1 then headers is obviously at 0 else headers may or may not be at 0\n",
    "                #Check if the first row in the csv is empty headers\n",
    "                if row_count>1:\n",
    "                    if not all(value.strip() == '' for value in df0.split(\"\\n\")[0].split(csv_seperator)): \n",
    "                        row_count=1\n",
    "                df=pd.read_csv(io.StringIO(df0), sep=csv_seperator, \n",
    "                               header=0 if row_count==1 else 1, keep_default_na=False) # read table with appropiate column headers\n",
    "                df.rename(columns=lambda x: '' if str(x).startswith('Unnamed:') else x, inplace=True) \n",
    "            table=df.to_csv(index=None, sep=csv_seperator)\n",
    "\n",
    "            if ids in table_page:\n",
    "                table_page[ids].append(table)\n",
    "            else:\n",
    "                table_page[ids]=[table]\n",
    "            # Extract table data and remaining content\n",
    "            pattern = re.compile(r'<table>(.*?)(</table>)', re.DOTALL) \n",
    "            data=item\n",
    "            table_match = re.search(pattern, data)\n",
    "            table_data = table_match.group(1) if table_match else '' \n",
    "            remaining_content = data[table_match.end():] if table_match else data            \n",
    "            content[idx]=f\"<<table>><table>{table}</table><</table>>\" ## attach xml tags to differentiate table from other text\n",
    "            count+=1\n",
    "            # Check for list items in remaining content\n",
    "            if \"<<list>>\" in remaining_content:\n",
    "                output=split_list_items_(remaining_content)\n",
    "                output=[x.strip() for x in output if x.strip()]\n",
    "                document_holder[ids].extend([content[idx]]+output)           \n",
    "            else:\n",
    "                document_holder[ids].extend([content[idx]]+[x.strip() for x in remaining_content.split('\\n') if x.strip()]) # split other text by new line to be independent items in the python list.\n",
    "        else:   \n",
    "            # Check for list items and tables in remaining content\n",
    "            if \"<<list>>\" in item and \"<table>\" not in item:   \n",
    "                output=split_list_items_(item)\n",
    "                output=[x.strip() for x in output if x.strip()]\n",
    "                document_holder[ids].extend(output)\n",
    "            else:\n",
    "                document_holder[ids].extend([x.strip() for x in item.split(\"\\n\") if x.strip()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f4f4ba-e1f4-4f9d-a7ec-49f9eb3ead44",
   "metadata": {},
   "source": [
    "Here we first flatten a nested list into a single list and then join its elements using newline characters. Subsequently, the string is split into segments based on the `<titles>` tag (split by title section hierarchy), generating a list of sub-section segments. Following this, the function `sub_header_content_splitta` is defined to process a string, splitting it by XML tags and extracting text segments, excluding segments containing specific XML tags such as `<header>`, `<list>`, or `<table>`. This function takes a string as input, applies a regular expression pattern to split it by XML tags, and iterates through the resulting segments to filter out those containing the specified XML tags. The extracted text segments are then returned as a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21eb2fde-7547-4390-831b-5c13b198d775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Flatten the nested list document_holder into a single list and Join the flattened list by \"\\n\"\n",
    "flattened_list = [item for sublist in document_holder.values() for item in sublist]\n",
    "result = \"\\n\".join( flattened_list)\n",
    "header_split=result.split(\"<titles>\")\n",
    "\n",
    "def sub_header_content_splitta(string):   \n",
    "    \"\"\"\n",
    "    Splits the input string by XML tags and returns a list containing the segments of text,\n",
    "    excluding segments containing specific XML tags such as \"<header>\", \"<list>\", or \"<table>\".\n",
    "\n",
    "    Parameters:\n",
    "    string (str): The input string to be processed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the segments of text extracted from the input string.\n",
    "    \"\"\" \n",
    "    pattern = re.compile(r'<<[^>]+>>')\n",
    "    segments = re.split(pattern, string)\n",
    "    result = []\n",
    "    for segment in segments:\n",
    "        if segment.strip():\n",
    "            if \"<header>\" not in segment and \"<list>\" not in segment and  \"<table>\" not in segment:\n",
    "                segment=[x.strip() for x in segment.split('\\n') if x.strip()]\n",
    "                result.extend(segment)\n",
    "            else:\n",
    "                result.append(segment)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd9cf4-ee87-4233-bd12-01f50c00b8b7",
   "metadata": {},
   "source": [
    "## Document Chunking\n",
    "This cell iterates through the document per title section and chunks content within each sub-sections in the following manner:\n",
    "- It uses number of words as chunking threshold.\n",
    "- It looks for the different xml tags to identify the different document content types. \n",
    "    - Iterating through the various sub-section within a section title identified by the **sub-section header** and only chunking contents within each sub-section. No chunk include multiple subsection content even if the max words threshold has not been met.\n",
    "    - If a table xml tag is found, it checks if there is a sentence before that table (the heueristics employed here is that the sentence before a table is usually the table header) and use it as table headers. It then splits table by rows until desired chunk is achieved and appends the corresponding section header to the table chunk.\n",
    "    - If a list is found, split list by items until desired chunk is achieved. Employ same heuristics as above and append list headers to all list chunk.\n",
    "    - For other text, it chunks by paragraphs and appends each sub-section header to the corresponding chunks.\n",
    "- A dicionary containing each complete sub-section is also stored to be used as metadata during indexing.\n",
    "- The complete table found in each chunk is also stored for metadata purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c8154-00b5-466b-b3f6-54f91cde19fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "max_words = 200\n",
    "chunks = {}\n",
    "table_header_dict={} \n",
    "chunk_header_mapping={}\n",
    "list_header_dict={}\n",
    "\n",
    "# iterate through each title section\n",
    "for title_ids, items in enumerate(header_split):\n",
    "    title_chunks = []\n",
    "    current_chunk = []\n",
    "    num_words = 0   \n",
    "    table_header_dict[title_ids]={}\n",
    "    chunk_header_mapping[title_ids]={}\n",
    "    list_header_dict[title_ids]={}\n",
    "    chunk_counter=0\n",
    "    for item_ids,item in enumerate(items.split('<headers>')): #headers\n",
    "        # print(\"\".join(current_chunk).strip())\n",
    "        lines=sub_header_content_splitta(item)             \n",
    "        SECTION_HEADER=None \n",
    "        TITLES=None\n",
    "        num_words = 0  \n",
    "        for ids_line,line in enumerate(lines): #header lines  \n",
    "            \n",
    "            if line.strip():\n",
    "                if \"<title>\" in line:   \n",
    "                    TITLES=re.findall(r'<title>(.*?)</title>', line)[0].strip()\n",
    "                    line=TITLES \n",
    "                    if re.sub(r'<[^>]+>', '', \"\".join(lines)).strip()==TITLES:\n",
    "                        chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "                        chunk_counter+=1\n",
    "                if \"<header>\" in line:   \n",
    "                    SECTION_HEADER=re.findall(r'<header>(.*?)</header>', line)[0].strip()\n",
    "                    line=SECTION_HEADER    \n",
    "                    first_header_portion=True\n",
    "                next_num_words = num_words + len(re.findall(r'\\w+', line))  \n",
    "\n",
    "                if  \"<table>\" not in line and \"<list>\" not in line:\n",
    "                    if next_num_words > max_words and \"\".join(current_chunk).strip()!=SECTION_HEADER and current_chunk and \"\".join(current_chunk).strip()!=TITLES:\n",
    "                \n",
    "                        if SECTION_HEADER :\n",
    "                            if first_header_portion:\n",
    "                                first_header_portion=False                                            \n",
    "                            else:\n",
    "                                current_chunk.insert(0, SECTION_HEADER.strip())                       \n",
    "                        \n",
    "                        title_chunks.append(current_chunk)                  \n",
    "                        chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "               \n",
    "                        current_chunk = []\n",
    "                        num_words = 0 \n",
    "                        chunk_counter+=1\n",
    "             \n",
    "                    current_chunk.append(line)    \n",
    "                    num_words += len(re.findall(r'\\w+', line))\n",
    "\n",
    "                \"\"\"\n",
    "                Goal is to segment out table items and chunks intelligently.\n",
    "                We chunk the table by rows and for each chunk of the table we append the table column headers\n",
    "                and table headers if any. This way we preserve the table information across each chunks.\n",
    "                This will help improve semantic search where all the chunks relating to a table would be in the \n",
    "                top k=n response giving the LLM mcomplet information on the table.\n",
    "                \"\"\"\n",
    "\n",
    "                if \"<table>\" in line:\n",
    "                    # Get table header which is usually line before table in document              \n",
    "                    line_index=lines.index(line)\n",
    "                    if line_index!=0 and \"<table>\" not in lines[line_index-1] and \"<list>\" not in lines[line_index-1]: #Check if table is first item on the page, then they wont be a header (header may be included it table) and also if table is the the last item in the list\n",
    "                        header=lines[line_index-1].replace(\"<header>\",\"\").replace(\"</header>\",\"\")\n",
    "                    else:\n",
    "                        header=\"\"                   \n",
    "              \n",
    "                    table = line.split(\"<table>\")[-1].split(\"</table>\")[0] # get table from demarcators              \n",
    "                    df=pd.read_csv(io.StringIO(table), sep=csv_seperator, keep_default_na=False,header=None)\n",
    "                    df.columns = df.iloc[0]\n",
    "                    df = df[1:]\n",
    "                    df.rename(columns=lambda x: '' if str(x).startswith('Unnamed:') else x, inplace=True)                    \n",
    "                    table_chunks = []\n",
    "                    curr_chunk = [df.columns.to_list()] #start current chunk with table column names    \n",
    "                    words=len(re.findall(r'\\w+', str(current_chunk)+\" \"+str(curr_chunk)))  \n",
    "                    # Iterate through the rows in the table\n",
    "                    for row in df.itertuples(index=False):\n",
    "                        curr_chunk.append(row)         \n",
    "                        words+=len(re.findall(r'\\w+', str(row)))\n",
    "                        if words > max_words:                        \n",
    "                            if [x for x in table_header_dict[title_ids] if chunk_counter == x]:\n",
    "                                table_header_dict[title_ids][chunk_counter].extend([header]+[table])\n",
    "                            else:\n",
    "                                table_header_dict[title_ids][chunk_counter]=[header]+[table]                            \n",
    "                            table_chunks.append(\"\\n\".join([csv_seperator.join(str(x) for x in curr_chunk[0])] + [csv_seperator.join(str(x) for x in r) for r in curr_chunk[1:]])) #join chunk lines together to for a csv \n",
    "                            tab_chunk=\"\\n\".join([csv_seperator.join(str(x) for x in curr_chunk[0])] + [csv_seperator.join(str(x) for x in r) for r in curr_chunk[1:]]) #join chunk lines together to for a csv\n",
    "                            words = len(re.findall(r'\\w+', str(curr_chunk[0]))) # set word count to word length of column header names\n",
    "                            if header: #If header  attach header to table                         \n",
    "                                if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower(): #check if header is in the chunk and remove to avoid duplicacy of header in chunk                        \n",
    "                                    current_chunk.pop(-1)\n",
    "                                # Append section header to table\n",
    "                                if SECTION_HEADER and SECTION_HEADER.lower().strip() != header.lower().strip():\n",
    "                                    if first_header_portion:\n",
    "                                        first_header_portion=False\n",
    "                                    else:\n",
    "                                        current_chunk.insert(0, SECTION_HEADER.strip())                             \n",
    "                                current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[tab_chunk]) #enrich table header with ':'\n",
    "                                title_chunks.append(current_chunk)                           \n",
    "                        \n",
    "                            else:\n",
    "                                if SECTION_HEADER:\n",
    "                                    if first_header_portion:\n",
    "                                        first_header_portion=False\n",
    "                                    else:\n",
    "                                        current_chunk.insert(0, SECTION_HEADER.strip())                                \n",
    "                                current_chunk.extend([tab_chunk])\n",
    "                                title_chunks.append(current_chunk)                        \n",
    "                            chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "                            chunk_counter+=1\n",
    "                            num_words=0\n",
    "                            current_chunk=[]\n",
    "                            curr_chunk = [curr_chunk[0]]\n",
    "                    \n",
    "                    if curr_chunk != [df.columns.to_list()] and lines.index(line) == len(lines)-1: #if table chunk still remaining and table is last item in page append as last chunk\n",
    "                        table_chunks.append(\"\\n\".join([csv_seperator.join(str(x) for x in curr_chunk[0])] + [csv_seperator.join(str(x) for x in r) for r in curr_chunk[1:]]))\n",
    "                        tab_chunk=\"\\n\".join([csv_seperator.join(str(x) for x in curr_chunk[0])] + [csv_seperator.join(str(x) for x in r) for r in curr_chunk[1:]])                        \n",
    "                        if [x for x in table_header_dict[title_ids] if chunk_counter == x]:\n",
    "                            table_header_dict[title_ids][chunk_counter].extend([header]+[table])\n",
    "                        else:\n",
    "                            table_header_dict[title_ids][chunk_counter]=[header]+[table]   \n",
    "                        \n",
    "                        if header: \n",
    "                            if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():#check if header is in the chunk and remove to avoid duplicacy of header in chunk\n",
    "                                current_chunk.pop(-1) \n",
    "                            if SECTION_HEADER and SECTION_HEADER.lower().strip() != header.lower().strip():\n",
    "                                if first_header_portion:\n",
    "                                    first_header_portion=False\n",
    "                                else:\n",
    "                                    current_chunk.insert(0, SECTION_HEADER.strip())                          \n",
    "                            current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[tab_chunk])\n",
    "                            title_chunks.append(current_chunk)                   \n",
    "                        else:\n",
    "                            if SECTION_HEADER:\n",
    "                                if first_header_portion:\n",
    "                                    first_header_portion=False\n",
    "                                else:\n",
    "                                    current_chunk.insert(0, SECTION_HEADER.strip())                            \n",
    "                            current_chunk.extend([tab_chunk])\n",
    "                            title_chunks.append(current_chunk)             \n",
    "                        chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "                        chunk_counter+=1\n",
    "                        num_words=0\n",
    "                        current_chunk=[]\n",
    "                    elif curr_chunk != [df.columns.to_list()] and lines.index(line) != len(lines)-1: #if table is not last item in page and max word threshold is not reached, send no next loop\n",
    "                        table_chunks.append(\"\\n\".join([csv_seperator.join(str(x) for x in curr_chunk[0])] + [csv_seperator.join(str(x) for x in r) for r in curr_chunk[1:]]))\n",
    "                        tab_chunk=\"\\n\".join([csv_seperator.join(str(x) for x in curr_chunk[0])] + [csv_seperator.join(str(x) for x in r) for r in curr_chunk[1:]])\n",
    "                        \n",
    "                        if [x for x in table_header_dict[title_ids] if chunk_counter == x]:\n",
    "                            table_header_dict[title_ids][chunk_counter].extend([header]+[table])\n",
    "                        else:\n",
    "                            table_header_dict[title_ids][chunk_counter]=[header]+[table]                         \n",
    "                        if header:               \n",
    "                            if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():#check if header is in the chunk and remove to avoid duplicacy of header in chunk\n",
    "                                current_chunk.pop(-1) \n",
    "                            current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[tab_chunk])\n",
    "                        else:\n",
    "                            current_chunk.extend([tab_chunk])                  \n",
    "                        num_words=words\n",
    "                     \n",
    "\n",
    "                \"\"\"\n",
    "                Goal is to segment out list items and chunk intelligently.\n",
    "                We chunk each list by items in the list and \n",
    "                for each list chunk we append the list header to the chunk to preserve the information of the list across chunks.\n",
    "                This would boost retrieval process where question pertaining to a list will have all list chunks within\n",
    "                the topK=n responses.\n",
    "                \"\"\"\n",
    "\n",
    "                if \"<list>\" in line:\n",
    "                    # Get list header which is usually line before list in document\n",
    "                    line_index=lines.index(line)\n",
    "                    if line_index!=0 and \"<table>\" not in lines[line_index-1] and \"<list>\" not in lines[line_index-1]: #Check if table or list is the previous item on the page, then they wont be a header\n",
    "                        header=lines[line_index-1].replace(\"<header>\",\"\").replace(\"</header>\",\"\")\n",
    "                    else:\n",
    "                        header=\"\"           \n",
    "                    list_pattern = re.compile(r'<list>(.*?)(?:</list>|$)', re.DOTALL)   ## Grab all list contents within the list xml tags        \n",
    "                    list_match = re.search(list_pattern, line)\n",
    "                    list_ = list_match.group(1)\n",
    "                    list_lines=list_.split(\"\\n\")                \n",
    "\n",
    "                    curr_chunk = []  \n",
    "                    words=len(re.findall(r'\\w+', str(current_chunk)))  #start word count from any existing chunk\n",
    "                    # Iterate through the items in the list\n",
    "                    for lyst_item in list_lines:\n",
    "                        curr_chunk.append(lyst_item)         \n",
    "                        words+=len(re.findall(r'\\w+', lyst_item)) \n",
    "                        if words >= max_words: # \n",
    "                            if [x for x in list_header_dict[title_ids] if chunk_counter == x]:\n",
    "                                list_header_dict[title_ids][chunk_counter].extend([header]+[list_])\n",
    "                            else:\n",
    "                                list_header_dict[title_ids][chunk_counter]=[header]+[list_]  \n",
    "                            words=0     \n",
    "                            list_chunk=\"\\n\".join(curr_chunk)\n",
    "                            if header: # attach list header                       \n",
    "                                if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():#check if header is in the chunk and remove to avoid duplicacy of header in chunk                        \n",
    "                                    current_chunk.pop(-1)  \n",
    "                                # Append section content header to list\n",
    "                                if SECTION_HEADER and SECTION_HEADER.lower().strip() != header.lower().strip():\n",
    "                                    if first_header_portion:\n",
    "                                        first_header_portion=False\n",
    "                                    else:\n",
    "                                        current_chunk.insert(0, SECTION_HEADER.strip())\n",
    "                                    \n",
    "                                current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[list_chunk]) \n",
    "                                title_chunks.append(current_chunk)                          \n",
    "                         \n",
    "                            else:\n",
    "                                if SECTION_HEADER:\n",
    "                                    if first_header_portion:\n",
    "                                        first_header_portion=False\n",
    "                                    else:\n",
    "                                        current_chunk.insert(0, SECTION_HEADER.strip())\n",
    "                                    \n",
    "                                current_chunk.extend([list_chunk])\n",
    "                                title_chunks.append(current_chunk)                            \n",
    "                            chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "                            chunk_counter+=1\n",
    "                            num_words=0\n",
    "                            current_chunk=[]\n",
    "                            curr_chunk = []\n",
    "                    if curr_chunk  and lines.index(line) == len(lines)-1: #if list chunk still remaining and list is last item in page append as last chunk\n",
    "                        list_chunk=\"\\n\".join(curr_chunk)\n",
    "                        if [x for x in list_header_dict[title_ids] if chunk_counter == x]:\n",
    "                            list_header_dict[title_ids][chunk_counter].extend([header]+[list_])\n",
    "                        else:\n",
    "                            list_header_dict[title_ids][chunk_counter]=[header]+[list_]  \n",
    "                        if header: \n",
    "                            if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower(): #check if header is in the chunk and remove to avoid duplicacy of header in chunk\n",
    "                                current_chunk.pop(-1)                            \n",
    "                            if SECTION_HEADER and SECTION_HEADER.lower().strip() != header.lower().strip():\n",
    "                                if first_header_portion:\n",
    "                                    first_header_portion=False\n",
    "                                else:\n",
    "                                    current_chunk.insert(0, SECTION_HEADER.strip())                   \n",
    "                            current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[list_chunk])\n",
    "                            title_chunks.append(current_chunk)                        \n",
    "                        else:\n",
    "                            if SECTION_HEADER:\n",
    "                                if first_header_portion:\n",
    "                                    first_header_portion=False\n",
    "                                else:\n",
    "                                    current_chunk.insert(0, SECTION_HEADER.strip())                   \n",
    "                            current_chunk.extend([list_chunk])\n",
    "                            title_chunks.append(current_chunk)                     \n",
    "                        chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "                        chunk_counter+=1\n",
    "                        num_words=0\n",
    "                        current_chunk=[]\n",
    "                    elif curr_chunk and lines.index(line) != len(lines)-1: #if list is not last item in page and max word threshold is not reached, send to next loop          \n",
    "                        list_chunk=\"\\n\".join(curr_chunk)\n",
    "                        if [x for x in list_header_dict[title_ids] if chunk_counter == x]:\n",
    "                            list_header_dict[title_ids][chunk_counter].extend([header]+[list_])\n",
    "                        else:\n",
    "                            list_header_dict[title_ids][chunk_counter]=[header]+[list_]  \n",
    "                        if header:               \n",
    "                            if current_chunk and current_chunk[-1].strip().lower()==header.strip().lower():#check if header is in the chunk and remove to avoid duplicacy of header in chunk\n",
    "                                current_chunk.pop(-1) \n",
    "                            current_chunk.extend([header.strip()+':' if not header.strip().endswith(':') else header.strip() ]+[list_chunk])\n",
    "                        else:\n",
    "                            current_chunk.extend([list_chunk])                  \n",
    "                        num_words=words\n",
    "\n",
    "\n",
    "        if current_chunk and \"\".join(current_chunk).strip()!=SECTION_HEADER and \"\".join(current_chunk).strip()!=TITLES:\n",
    "    \n",
    "            if SECTION_HEADER:\n",
    "                if first_header_portion:\n",
    "                    first_header_portion=False\n",
    "                else:\n",
    "                    current_chunk.insert(0, SECTION_HEADER.strip())         \n",
    "            title_chunks.append(current_chunk)\n",
    "            chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "            current_chunk=[]\n",
    "            chunk_counter+=1\n",
    "    if current_chunk:\n",
    "  \n",
    "        title_chunks.append(current_chunk) \n",
    "        chunk_header_mapping[title_ids][chunk_counter]=lines\n",
    "    chunks[title_ids] = title_chunks\n",
    "       \n",
    "    for title_id, title_chunks in chunks.items():\n",
    "        for chunk_id, chunk in enumerate(title_chunks):\n",
    "            # Create a key for the chunk file\n",
    "            key = f'chunk/title_{title_id}/chunk_{chunk_id}.txt'\n",
    "\n",
    "            # Convert the chunk list to a string\n",
    "            chunk_str = '\\n'.join(chunk)\n",
    "\n",
    "            # Upload the chunk to S3\n",
    "            s3.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=key,\n",
    "                Body=chunk_str.encode('utf-8')\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9501b756-ba00-48f7-a400-49bae1d834d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0:\n",
      "Health Insurance Build Charts\n",
      "<title>Health Insurance Build Charts </title>:\n",
      "1. If there has been weight loss of more than 20 pounds within one year, divide the loss in half and add it to current weight before entering into the table. \n",
      "2. A reduction in rating due to build will be considered once an insured loses enough to qualify for the lower rating and maintains the reduced weight for at least 6-12 months. \n",
      "3. Underweight can be more serious than overweight. Keep in mind that in certain people, because of small physical stature, an underweight condition is normal and perfectly healthy. \n",
      "4. Sudden weight loss without voluntary dieting is an ominous sign. \n",
      "5. Certain conditions require an additional rating because of the enhanced morbidity risk, e.g., hypertension and overweight build. \n",
      "6. The weight is in pounds.\n",
      "Height|Height|MALE|MALE|MALE|MALE|MALE|MALE|Height|Height|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE\n",
      "F \n",
      "T E E|I \n",
      "H N C|20% for \n",
      "Weights than less|Weight Avg.|Premium Percentage Increase in|Premium Percentage Increase in|Premium Percentage Increase in|Premium Percentage Increase in|F \n",
      "T E E|I \n",
      "N H C|20% for \n",
      "Weights than less|Weight Avg.|Premium Percentage Increase in|Premium Percentage Increase in|Premium Percentage Increase in|Premium Percentage Increase in\n",
      "\n",
      "\n",
      "Chunk 1:\n",
      "Height|Height|MALE|MALE|MALE|MALE|MALE|MALE|Height|Height|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE\n",
      "F \n",
      "T E E|I \n",
      "H N C|20% for \n",
      "Weights than less|Weight Avg.|20%|40%|80% +ER|Decline|F \n",
      "T E E|I \n",
      "N H C|20% for \n",
      "Weights than less|Weight Avg.|20%|40%|80% +ER|Decline\n",
      "5|0|90|129|167-183|184-195|196-208|209+|4|8|75|107|148-163|164-173|174-184|185+\n",
      "5|1|93|133|173-189|190-201|202-214|215+|4|9|77|110|153-168|169-178|179-189|190+\n",
      "5|2|97|138|180-196|197-209|210-223|224+|4|10|79|113|157-172|173-182|183-194|195+\n",
      "5|3|100|143|186-203|204-216|217-231|232+|4|11|81|115|160-175|176-185|186-198|199+\n",
      "\n",
      "\n",
      "Chunk 2:\n",
      "Height|Height|MALE|MALE|MALE|MALE|MALE|MALE|Height|Height|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE\n",
      "5|4|103|147|192-209|210-222|223-237|238+|5|0|83|118|164-180|181-191|192-203|204+\n",
      "5|5|106|151|197-215|216-229|230-244|245+|5|1|85|121|169-185|186-196|197-208|209+\n",
      "5|6|109|156|204-222|223-236|237-252|253+|5|2|87|124|173-189|190-201|202-214|215+\n",
      "5|7|112|160|210-228|229-242|243-258|259+|5|3|90|128|179-196|197-207|208-220|221+\n",
      "5|8|116|165|216-235|236-250|251-266|267+|5|4|92|131|183-200|201-212|213-226|227+\n",
      "\n",
      "\n",
      "Chunk 3:\n",
      "Height|Height|MALE|MALE|MALE|MALE|MALE|MALE|Height|Height|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE\n",
      "5|9|119|170|223-243|244-258|259-274|275+|5|5|94|134|188-205|206-217|218-231|232+\n",
      "5|10|122|174|229-248|249-264|265-281|282+|5|6|96|137|192-210|211-222|223-236|237+\n",
      "5|11|125|179|235-256|257-272|273-289|290+|5|7|99|141|198-216|217-229|230-243|244+\n",
      "6|0|128|184|242-263|264-279|280-297|298+|5|8|102|145|204-222|223-235|236-250|251+\n",
      "6|1|131|190|250-272|273-289|290-307|308+|5|9|105|150|211-230|231-244|245-259|260+\n",
      "\n",
      "\n",
      "Chunk 4:\n",
      "Height|Height|MALE|MALE|MALE|MALE|MALE|MALE|Height|Height|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE\n",
      "6|2|134|195|257-279|280-296|297-315|316+|5|10|107|153|215-235|236-248|249-264|265+\n",
      "6|3|138|201|265-287|288-306|307-325|326+|5|11|111|159|224-244|245-258|259-274|275+\n",
      "6|4|142|206|272-295|296-313|314-333|334+|6|0|115|164|231-252|253-267|268-283|284+\n",
      "6|5|145|211|279-302|303-321|322-341|342+|6|1|118|168|237-258|259-273|274-290|291+\n",
      "6|6|150|217|287-311|312-330|331-351|352+|6|2|120|172|243-264|265-280|281-297|298+\n",
      "\n",
      "\n",
      "Chunk 5:\n",
      "Height|Height|MALE|MALE|MALE|MALE|MALE|MALE|Height|Height|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE\n",
      "6|7|152|223|295-319|320-339|340-360|361+|6|3|123|176|249-271|272-286|287-303|304+\n",
      "6|8|158|228|302-327|328-347|348-368|369+|6|4|127|181|256-278|279-295|296-312|313+\n",
      "The Health Insurance Build Chart is a guide to the rating action World Insurance Company will take regarding weight The percentage increases assume that there are no other impairments present. If other impairments are found, the judgment of the underwriter will determine what action will be taken. Weights greater than those in the chart will render an applicant uninsurable for health coverage.\n",
      "The Health Insurance Build Chart is for use only with insureds and dependents age 15 or over. Cases involving overweight dependents under age 15 will be considered indi- vidually by the underwriter in consultation with the Medical Director.\n",
      "\n",
      "\n",
      "Chunk 6:\n",
      "Premium increases because of weight will be reconsidered in accordance with the following guidelines:\n",
      "1. If the increase is 20% or less, the certificate/policy must be in force at least six months. \n",
      "2. If the increase is more than 20%, the certificate/policy must be in force at least one year. \n",
      "3. In either case the weight loss must be maintained for more than six months. World must be furnished, at the expense of the insured, a statement from a physician or testing facility showing the insured's current height and weight.\n",
      "Health\n",
      "Underwriting\n",
      "Guide\n",
      "9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print chunks per title section\n",
    "for i, chunk in enumerate(chunks[2][:10], start=0):\n",
    "    print(f'Chunk {i}:')\n",
    "    for item in chunk:\n",
    "        print(item)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c665aeba-cb90-40c4-b40b-367cd5874a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preferred Underwriting Guidelines \n",
      "Health Insurance Build Charts \n",
      "Juvenile Build Charts \n",
      "Occupations Not Eligible for Health Insurance \n",
      "Non-Medical Guidelines \n",
      "Declinations \n",
      "Declinable Medications \n",
      "Common Medications/Therapeutic Use Reference \n",
      "Medical Underwriting Guidelines \n",
      "Health Underwriting Guide 18 \n",
      "19 \n",
      "20 \n",
      "21 \n",
      "22 \n",
      "23 \n",
      "24 \n",
      "Health Underwriting Guide 26 \n",
      "27 \n",
      "28 \n",
      "29 \n",
      "30 \n",
      "31 \n",
      "Health Underwriting Guide 32 \n"
     ]
    }
   ],
   "source": [
    "# List of title header sections document was split into\n",
    "for x in chunk_header_mapping:\n",
    "    if chunk_header_mapping[x]:\n",
    "        try:\n",
    "            title_pattern = re.compile(r'<title>(.*?)(?:</title>|$)', re.DOTALL)       \n",
    "            title_match = re.search(title_pattern, chunk_header_mapping[x][0][0])\n",
    "            title_ = title_match.group(1) if title_match else \"\"\n",
    "            print(title_, end='\\n')\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123dba4-c57c-4fe2-85f5-b17b4bf0d29b",
   "metadata": {},
   "source": [
    "Upload section contents (title and headers for each chunk) to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "445e5827-3821-4c59-9e02-64c06db499e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open (f\"{doc_id}.json\", \"w\") as f:\n",
    "    json.dump(chunk_header_mapping,f)\n",
    "s3.upload_file(f\"{doc_id}.json\", BUCKET, f\"{doc_id}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec2ac60-137c-4679-a0de-446e9f531008",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e463be44-d6b7-485d-a53d-023d88cdc209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "vector_store_name = f'bedrock-sample-rag-{suffix}'\n",
    "index_name = f\"bedrock-sample-rag-index-{suffix}\"\n",
    "aoss_client = boto3_session.client('opensearchserverless')\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role(bucket_name=bucket_name)\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e120a403-748c-4f2e-ad32-9299afb087d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f541ff07-7498-462c-a582-5d55f840d844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'ResponseMetadata': { 'HTTPHeaders': { 'connection': 'keep-alive',\n",
      "                                         'content-length': '314',\n",
      "                                         'content-type': 'application/x-amz-json-1.0',\n",
      "                                         'date': 'Thu, 23 May 2024 14:32:07 '\n",
      "                                                 'GMT',\n",
      "                                         'x-amzn-requestid': '6efa0aed-898c-4e53-96f0-c12dcef57398'},\n",
      "                        'HTTPStatusCode': 200,\n",
      "                        'RequestId': '6efa0aed-898c-4e53-96f0-c12dcef57398',\n",
      "                        'RetryAttempts': 0},\n",
      "  'createCollectionDetail': { 'arn': 'arn:aws:aoss:us-west-2:533356244334:collection/m2lfe3wceui8dktho4b0',\n",
      "                              'createdDate': 1716474727117,\n",
      "                              'id': 'm2lfe3wceui8dktho4b0',\n",
      "                              'kmsKeyArn': 'auto',\n",
      "                              'lastModifiedDate': 1716474727117,\n",
      "                              'name': 'bedrock-sample-rag-707',\n",
      "                              'standbyReplicas': 'ENABLED',\n",
      "                              'status': 'CREATING',\n",
      "                              'type': 'VECTORSEARCH'}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "566a837b-bdaf-4750-9b50-921a27a5c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m2lfe3wceui8dktho4b0.us-west-2.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "775bc585-b808-434e-9cf3-cda97d91c893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection...\n",
      "\n",
      "Collection successfully created:\n",
      "[{'arn': 'arn:aws:aoss:us-west-2:533356244334:collection/m2lfe3wceui8dktho4b0', 'collectionEndpoint': 'https://m2lfe3wceui8dktho4b0.us-west-2.aoss.amazonaws.com', 'createdDate': 1716474727117, 'dashboardEndpoint': 'https://m2lfe3wceui8dktho4b0.us-west-2.aoss.amazonaws.com/_dashboards', 'id': 'm2lfe3wceui8dktho4b0', 'kmsKeyArn': 'auto', 'lastModifiedDate': 1716474753816, 'name': 'bedrock-sample-rag-707', 'standbyReplicas': 'ENABLED', 'status': 'ACTIVE', 'type': 'VECTORSEARCH'}]\n"
     ]
    }
   ],
   "source": [
    "# wait for collection creation\n",
    "response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "# Periodically check collection status\n",
    "while (response['collectionDetails'][0]['status']) == 'CREATING':\n",
    "    print('Creating collection...')\n",
    "    time.sleep(30)\n",
    "    response = aoss_client.batch_get_collection(names=[vector_store_name])\n",
    "print('\\nCollection successfully created:')\n",
    "print(response[\"collectionDetails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9fc5b4a9-583a-4868-a6d5-9db3d3052e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opensearch serverless arn:  arn:aws:iam::533356244334:policy/AmazonBedrockOSSPolicyForKnowledgeBase_871\n"
     ]
    }
   ],
   "source": [
    "# create oss policy and attach it to Bedrock execution role\n",
    "create_oss_policy_attach_bedrock_execution_role(collection_id=collection_id,\n",
    "                                                bedrock_kb_execution_role=bedrock_kb_execution_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4130d8b8-af4b-411d-a95f-884d818df324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = auth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"bedrock-sample-index-{suffix}\"\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\",\n",
    "       \"number_of_shards\": 1,\n",
    "       \"knn.algo_param.ef_search\": 512,\n",
    "       \"number_of_replicas\": 0,\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1536,\n",
    "             \"method\": {\n",
    "                 \"name\": \"hnsw\",\n",
    "                 \"engine\": \"faiss\"\n",
    "             },\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60c02db8-3fb8-4f76-b5ac-a835a46da31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating index:\n",
      "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'bedrock-sample-index-707'}\n"
     ]
    }
   ],
   "source": [
    "# Create index\n",
    "response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "print('\\nCreating index:')\n",
    "print(response)\n",
    "time.sleep(60) # index creation can take up to a minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb2f4ec5-a72f-46ba-b05a-0e93fab15be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"vectorIndexName\": index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"NONE\" #Set to no chunking since we are manually chunking based on the document\n",
    "}\n",
    "\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",\n",
    "    \"inclusionPrefixes\":[\"chunk/\"] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "}\n",
    "\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "\n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Insurance Underwriting KB.\"\n",
    "roleArn = bedrock_kb_execution_role_arn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2dabe9e5-5225-4b69-9ae1-6ca42dea0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KnowledgeBase\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name = name,\n",
    "        description = description,\n",
    "        roleArn = roleArn,\n",
    "        knowledgeBaseConfiguration = {\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration = {\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "        }\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08db25e9-8a96-4789-97dd-e56ea23424c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2315a0ef-3bb9-4812-8a00-34a149dbde26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 5, 23, 14, 34, 38, 853420, tzinfo=tzlocal()),\n",
      "  'description': 'Insurance Underwriting KB.',\n",
      "  'knowledgeBaseArn': 'arn:aws:bedrock:us-west-2:533356244334:knowledge-base/A7WJNFHDI8',\n",
      "  'knowledgeBaseConfiguration': { 'type': 'VECTOR',\n",
      "                                  'vectorKnowledgeBaseConfiguration': { 'embeddingModelArn': 'arn:aws:bedrock:us-west-2::foundation-model/amazon.titan-embed-text-v1'}},\n",
      "  'knowledgeBaseId': 'A7WJNFHDI8',\n",
      "  'name': 'bedrock-sample-knowledge-base-707',\n",
      "  'roleArn': 'arn:aws:iam::533356244334:role/AmazonBedrockExecutionRoleForKnowledgeBase_871',\n",
      "  'status': 'CREATING',\n",
      "  'storageConfiguration': { 'opensearchServerlessConfiguration': { 'collectionArn': 'arn:aws:aoss:us-west-2:533356244334:collection/m2lfe3wceui8dktho4b0',\n",
      "                                                                   'fieldMapping': { 'metadataField': 'text-metadata',\n",
      "                                                                                     'textField': 'text',\n",
      "                                                                                     'vectorField': 'vector'},\n",
      "                                                                   'vectorIndexName': 'bedrock-sample-index-707'},\n",
      "                            'type': 'OPENSEARCH_SERVERLESS'},\n",
      "  'updatedAt': datetime.datetime(2024, 5, 23, 14, 34, 38, 853420, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f015a857-70b3-4819-bdd6-04da1672721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get KnowledgeBase \n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "653eb13f-0127-4815-b4ba-ed74b1a9a3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'createdAt': datetime.datetime(2024, 5, 23, 14, 34, 39, 450973, tzinfo=tzlocal()),\n",
      "  'dataSourceConfiguration': { 's3Configuration': { 'bucketArn': 'arn:aws:s3:::textract-2-kb-ws-us-west-2-533356244334',\n",
      "                                                    'inclusionPrefixes': [ 'chunk/']},\n",
      "                               'type': 'S3'},\n",
      "  'dataSourceId': 'QRVNWSBYEO',\n",
      "  'description': 'Insurance Underwriting KB.',\n",
      "  'knowledgeBaseId': 'A7WJNFHDI8',\n",
      "  'name': 'bedrock-sample-knowledge-base-707',\n",
      "  'status': 'AVAILABLE',\n",
      "  'updatedAt': datetime.datetime(2024, 5, 23, 14, 34, 39, 450973, tzinfo=tzlocal()),\n",
      "  'vectorIngestionConfiguration': { 'chunkingConfiguration': { 'chunkingStrategy': 'NONE'}}}\n"
     ]
    }
   ],
   "source": [
    "# Create a DataSource in KnowledgeBase \n",
    "create_ds_response = bedrock_agent_client.create_data_source(\n",
    "    name = name,\n",
    "    description = description,\n",
    "    knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "    dataSourceConfiguration = {\n",
    "        \"type\": \"S3\",\n",
    "        \"s3Configuration\":s3Configuration\n",
    "    },\n",
    "    vectorIngestionConfiguration = {\n",
    "        \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "    }\n",
    ")\n",
    "ds = create_ds_response[\"dataSource\"]\n",
    "pp.pprint(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e79892c1-bce3-4085-acd9-d354bf30bf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '9494a12f-f4e7-49db-9ee0-69bda636b45c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Thu, 23 May 2024 14:34:39 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '545',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '9494a12f-f4e7-49db-9ee0-69bda636b45c',\n",
       "   'x-amz-apigw-id': 'YOoP9GLJvHcEmAA=',\n",
       "   'x-amzn-trace-id': 'Root=1-664f53ff-3b2290673b45a2c04a3a0404'},\n",
       "  'RetryAttempts': 0},\n",
       " 'dataSource': {'knowledgeBaseId': 'A7WJNFHDI8',\n",
       "  'dataSourceId': 'QRVNWSBYEO',\n",
       "  'name': 'bedrock-sample-knowledge-base-707',\n",
       "  'status': 'AVAILABLE',\n",
       "  'description': 'Insurance Underwriting KB.',\n",
       "  'dataSourceConfiguration': {'type': 'S3',\n",
       "   's3Configuration': {'bucketArn': 'arn:aws:s3:::textract-2-kb-ws-us-west-2-533356244334',\n",
       "    'inclusionPrefixes': ['chunk/']}},\n",
       "  'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'NONE'}},\n",
       "  'createdAt': datetime.datetime(2024, 5, 23, 14, 34, 39, 450973, tzinfo=tzlocal()),\n",
       "  'updatedAt': datetime.datetime(2024, 5, 23, 14, 34, 39, 450973, tzinfo=tzlocal())}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get DataSource \n",
    "bedrock_agent_client.get_data_source(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1291cc4-63a6-4068-830a-42fce46b8856",
   "metadata": {},
   "source": [
    "### Start ingestion job\n",
    "Once the KB and data source is created, we can start the ingestion job.\n",
    "During the ingestion job, KB will fetch the documents in the data source, pre-process it to extract text, chunk it based on the chunking size provided, create embeddings of each chunk and then write it to the vector database, in this case OSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ab3290aa-33e7-4c97-8fcf-b185473ea27d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print KB ID\n",
      "'A7WJNFHDI8'\n"
     ]
    }
   ],
   "source": [
    "# Start an ingestion job\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])\n",
    "job = start_job_response[\"ingestionJob\"]\n",
    "pp.pprint(job)\n",
    "\n",
    "#  Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "      get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "          knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "            dataSourceId = ds[\"dataSourceId\"],\n",
    "            ingestionJobId = job[\"ingestionJobId\"]\n",
    "      )\n",
    "job = get_job_response[\"ingestionJob\"]\n",
    "pp.pprint(job)\n",
    "time.sleep(40)\n",
    "print(\"Print KB ID\")\n",
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "pp.pprint(kb_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "74fe91a0-80a2-4cca-9705-620280929c63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kb_id' (str)\n"
     ]
    }
   ],
   "source": [
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1e77e-4cbc-457d-a031-a8ba3472ead4",
   "metadata": {},
   "source": [
    "### Try out KB using Retrieve API\n",
    "Retrieve API retrieves the most relavant chunks allowing you to choose between passing the chunks directly to the model or to perform post processing tecniques such as re-ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "65e6abbc-bfab-46cb-a312-e12305ca57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out KB using Retrieve\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "model_id = \"anthropic.claude-v2\" # try with both claude instant as well as claude-v2. for claude v2 - \"anthropic.claude-v2\"\n",
    "model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "65900572-b743-4ca5-b149-cfe1a88cf6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "query = \"What is The Health Insurance Build Chart?\" #Type your query here. \n",
    "\n",
    "\n",
    "# retreive api for fetching only the relevant context.\n",
    "relevant_documents = bedrock_agent_runtime_client.retrieve(\n",
    "    retrievalQuery= {\n",
    "        'text': query\n",
    "    },\n",
    "    knowledgeBaseId=kb_id,\n",
    "    retrievalConfiguration= {\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 3 # will fetch top 3 documents which matches closely with the query.\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d2a0dfd-c1a9-4ac2-ba48-b2993e8a9138",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height|Height|MALE|MALE|MALE|MALE|MALE|MALE|Height|Height|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE\n",
      "6|7|152|223|295-319|320-339|340-360|361+|6|3|123|176|249-271|272-286|287-303|304+\n",
      "6|8|158|228|302-327|328-347|348-368|369+|6|4|127|181|256-278|279-295|296-312|313+\n",
      "The Health Insurance Build Chart is a guide to the rating action World Insurance Company will take regarding weight The percentage increases assume that there are no other impairments present. If other impairments are found, the judgment of the underwriter will determine what action will be taken. Weights greater than those in the chart will render an applicant uninsurable for health coverage.\n",
      "The Health Insurance Build Chart is for use only with insureds and dependents age 15 or over. Cases involving overweight dependents under age 15 will be considered indi- vidually by the underwriter in consultation with the Medical Director.\n",
      "Health Insurance Build Charts\n",
      ":\n",
      "1. If there has been weight loss of more than 20 pounds within one year, divide the loss in half and add it to current weight before entering into the table. \n",
      "2. A reduction in rating due to build will be considered once an insured loses enough to qualify for the lower rating and maintains the reduced weight for at least 6-12 months. \n",
      "3. Underweight can be more serious than overweight. Keep in mind that in certain people, because of small physical stature, an underweight condition is normal and perfectly healthy. \n",
      "4. Sudden weight loss without voluntary dieting is an ominous sign. \n",
      "5. Certain conditions require an additional rating because of the enhanced morbidity risk, e.g., hypertension and overweight build. \n",
      "6. The weight is in pounds.\n",
      "Height|Height|MALE|MALE|MALE|MALE|MALE|MALE|Height|Height|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE|FEMALE\n",
      "F \n",
      "T E E|I \n",
      "H N C|20% for \n",
      "Weights than less|Weight Avg.|Premium Percentage Increase in|Premium Percentage Increase in|Premium Percentage Increase in|Premium Percentage Increase in|F \n",
      "T E E|I \n",
      "N H C|20% for \n",
      "Weights than less|Weight Avg.|Premium Percentage Increase in|Premium Percentage Increase in|Premium Percentage Increase in|Premium Percentage Increase in\n",
      "Build Chart for Preferred Risks\n",
      "Guidelines - To be eligible for Preferred Rates, the proposed insured and/or proposed insured spouse:\n",
      "1. Must be between the ages of 18 and 60; \n",
      "2. Must not have an added health exclusion rider or health rate-up; \n",
      "3. Must fall within the applicable height/weight table; and \n",
      "4. Must answer \"no\" to all questions on preferred questionnaire;\n",
      "Male|Male|Female|Female\n",
      "Height|Weight|Height|Weight\n",
      "5'0\"|98-152|4'10\"|90-138\n",
      "5'1\"|101-155|4'11\"|92-140\n",
      "5'2\"|103-159|5'0\"|94-143\n",
      "5'3\"|105-162|5'1\"|96-146\n",
      "5'4\"|107-166|5'2\"|98-150\n",
      "5'5\"|110-171|5'3\"|101-153\n",
      "5'6\"|112-175|5'4\"|104-158\n",
      "5'7\"|115-181|5'5\"|107-163\n",
      "5'8\"|118-186|5'6\"|109-168\n",
      "5'9\"|121-191|5'7\"|112-173\n"
     ]
    }
   ],
   "source": [
    "passage = relevant_documents['retrievalResults']\n",
    "contextList=[]\n",
    "for content in passage:\n",
    "    contextList.append(content['content']['text'])\n",
    "    print(content['content']['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7c3cc-beb5-48e2-abaa-66b46c5e8373",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b31839-bd89-4948-a569-ac38a60bbac5",
   "metadata": {},
   "source": [
    "#### RERANKING (OPTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafcfb4e-e454-4e94-9cb8-64cec07a3606",
   "metadata": {},
   "source": [
    "This section touches on Reranking. It uses the deployed Sagemaker Jumpstart [BGE M3 model](https://huggingface.co/BAAI/bge-m3). To use this code logic you first have to deploy the model to an endpoint. After which you collect the endpoint name and replace in the logic.\n",
    "\n",
    "**NOTE**: You can skip this section if you do not have a deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56978d3f-88e7-48c2-8785-271317eb97f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb0e6040-b6e1-436c-b91a-504ea79f97ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank_cross_encoder(query, docs):\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    # Create pairs as strings\n",
    "    pairs = [[query, doc_text] for doc_text in docs]\n",
    "\n",
    "    # Predict scores for pairs\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    # Get the indices of the chunks with the highest scores\n",
    "    n = 2  # Number of top chunks to keep\n",
    "    top_indices = np.argsort(scores)[::-1][:n]\n",
    "\n",
    "    # Put the chunks in a list\n",
    "    reranked_docs = [docs[idx] for idx in top_indices]\n",
    "\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95fbc6fe-adec-47fa-a9aa-359c557bfef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reranked_passages=rerank_cross_encoder(query,contextList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e4258-95a2-4000-97e1-12659f4eef3a",
   "metadata": {},
   "source": [
    "## Bedrock Anthropic LLM Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1161d-5e69-4832-9d27-a8e417a2d4a8",
   "metadata": {},
   "source": [
    "Using the a prompt template with placeholders for the retrieved passages as `passages` under **document** tags and any retrieved standalone tables and list found within each retrieved passages as `tab` under **additional_information** tags.\\\n",
    "Change the `csv_seperator` variable name to what was used during chunking. default is \"|\" pipe character.\\\n",
    "Anthropic Claude models (Claude 3 and 2) is used to generate a response to the user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "edc0c8ab-5cf5-4200-829f-08c576db2d45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Size of prompt token is 768\n"
     ]
    }
   ],
   "source": [
    "csv_seperator=\"|\"\n",
    "prompt_template=f\"\"\"You are a helpful, obedient and truthful insurance underwriting assistance.\n",
    "\n",
    "<document>\n",
    "{reranked_passages}\n",
    "</document>          \n",
    "\n",
    "\n",
    "<instructions>\n",
    "When providing your response based on the document:\n",
    "1. Understand the question to know what is being asked of you.\n",
    "2. Review the entire document provided and check if it contains relevant information to answer the question. Only pay attention to passages with relevant information.\n",
    "3. Any tables provided within the document or additional information are delimited by {csv_seperator} character.\n",
    "4. If the document is sufficient to answer the question, provide a comprehensive answer ENTIRELY based on the document provided. DO NOT make up answers not present in the document.\n",
    "5. If the answer is not available in the document, say so.\n",
    "</instructions>\n",
    "\n",
    "Question: {query}\n",
    "if able to answer:\n",
    "    Include in your response before your answer:    \n",
    "    <source>document or additional info tag(s) containing the relevant info</source>\"\"\"\n",
    "\n",
    "print(f' Size of prompt token is {client.count_tokens(prompt_template)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a878e4c1-b90a-4853-9e09-073408bbc2ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<source>['The Health Insurance Build Chart is a guide to the rating action World Insurance Company will take regarding weight The percentage increases assume that there are no other impairments present. If other impairments are found, the judgment of the underwriter will determine what action will be taken. Weights greater than those in the chart will render an applicant uninsurable for health coverage.', 'The Health Insurance Build Chart is for use only with insureds and dependents age 15 or over. Cases involving overweight dependents under age 15 will be considered indi- vidually by the underwriter in consultation with the Medical Director.']</source>\n",
      "\n",
      "The Health Insurance Build Chart is a guide used by World Insurance Company to determine rating actions (such as percentage increases in premiums) based on an applicant's weight. The chart provides weight ranges for different heights and genders, and indicates the corresponding premium increase percentages. Weights greater than the maximum in the chart would render an applicant uninsurable for health coverage. The chart is intended for use with insureds and dependents aged 15 or over, while cases involving overweight dependents under 15 are handled individually by the underwriter in consultation with the Medical Director.\n",
      "Input Tokens: 830\n",
      "Output Tokens: 261\n"
     ]
    }
   ],
   "source": [
    "model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-3-sonnet-20240229-v1:0\"\"anthropic.claude-v2\",\"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "model_response,input_tokens, output_tokens=_invoke_bedrock_with_retries([], \"\", prompt_template,model_id , [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59db78a-71ce-418b-9566-fc7ca8167490",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ecd86-b1a9-4fb7-8b99-f5d382f53d0d",
   "metadata": {},
   "source": [
    "This notebook showcases the extraction of content from a document while maintaining its layout structure. Additionally, we processed and chunked the document, ensuring the integrity of the information was preserved. Furthermore, we indexed these chunks and associated hierarchical metadata information, offering flexibility in information retrieval. \n",
    "\n",
    "Finally, we conducted a RAG query and generated contextual answers."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
